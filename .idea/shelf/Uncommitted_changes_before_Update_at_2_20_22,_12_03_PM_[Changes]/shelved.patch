Index: src/main/scala/Utilities.scala
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import scala.io.StdIn.readLine\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions.col\nimport org.apache.spark.storage.StorageLevel\n\nimport java.io.{File, PrintWriter}\nimport scala.Console.println\nimport P2.{spark, _}\n\nobject Utilities {\n  def junk(spark: SparkSession): Unit = {\n    spark.sql(\n      \"set hive.exec.dynamic.partition.mode=nonstrict\"\n    ) // TODO USE THIS FOR A MORE COMPACT DELETE\n\n    //spark.sql(\"DROP TABLE IF EXISTS branchbevs\")\n    //spark.sql(\"DROP TABLE IF EXISTS cons_a\")\n    //spark.sql(\"DROP TABLE IF EXISTS cons_b\")\n    //spark.sql(\"DROP TABLE IF EXISTS cons_c\")\n    //spark.sql(\"DROP TABLE IF EXISTS cons_aXb\")\n    //spark.sql(\"DROP TABLE IF EXISTS constot1\")\n    //spark.sql(\"DROP TABLE IF EXISTS constot2\")\n    //spark.sql(\"DROP TABLE IF EXISTS constot3\")\n    //spark.sql(\"DROP TABLE IF EXISTS constot4\")\n    //spark.sql(\"DROP TABLE IF EXISTS constot5\")\n    //spark.sql(\"DROP TABLE IF EXISTS constot6\")\n    //spark.sql(\"DROP TABLE IF EXISTS constot7\")\n    //spark.sql(\"DROP TABLE IF EXISTS constot8\")\n    //spark.sql(\"DROP TABLE IF EXISTS constot9\")\n    //spark.sql(\"DROP TABLE IF EXISTS constotall\")\n    //spark.sql(\"DROP TABLE IF EXISTS cons_tot_all\")\n\n//    spark.sql(\"CREATE TABLE IF NOT EXISTS branch_a (bev STRING, branch STRING)\" +\n//        \"ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\")\n//    spark.sql(\"CREATE TABLE IF NOT EXISTS branch_b (bev STRING, branch STRING)\" +\n//      \"ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\")\n//    spark.sql(\"CREATE TABLE IF NOT EXISTS branch_c (bev STRING, branch STRING)\" +\n//      \"ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\")\n//    spark.sql(\"CREATE TABLE IF NOT EXISTS cons_a (bev STRING, count INT)\" +\n//      \"ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\")\n//    spark.sql(\"CREATE TABLE IF NOT EXISTS cons_a (bev STRING, count INT)\" +\n//      \"ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\")\n//    spark.sql(\"CREATE TABLE IF NOT EXISTS cons_b (bev STRING, count INT)\" +\n//      \"ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\")\n//    spark.sql(\"CREATE TABLE IF NOT EXISTS cons_c (bev STRING, count INT)\" +\n//      \"ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\")\n//    spark.sql(\"CREATE TABLE IF NOT EXISTS Partitioned_abc(bev STRING) PARTITIONED BY (branches STRING)\")\n\n//    spark.sql(\"LOAD DATA LOCAL INPATH 'input/Bev_BranchA.txt' OVERWRITE INTO TABLE branch_a\")\n//    spark.sql(\"LOAD DATA LOCAL INPATH 'input/Bev_BranchB.txt' INTO TABLE branch_b\")\n//    spark.sql(\"LOAD DATA LOCAL INPATH 'input/Bev_BranchC.txt' INTO TABLE branch_c\")\n    //    spark.sql(\"LOAD DATA LOCAL INPATH 'input/Bev_ConscountA.txt' INTO TABLE cons_a\")\n    //    spark.sql(\"LOAD DATA LOCAL INPATH 'input/Bev_ConscountA.txt' INTO TABLE cons_a\")\n    //    spark.sql(\"LOAD DATA LOCAL INPATH 'input/Bev_ConscountB.txt' INTO TABLE cons_b\")\n    //    spark.sql(\"LOAD DATA LOCAL INPATH 'input/Bev_ConscountC.txt' INTO TABLE cons_c\")\n//    spark.sql(\"INSERT OVERWRITE TABLE Partitioned_abc PARTITION(branches) SELECT bev,branch FROM all_branch\")\n\n//    Intersection of cons_a and cons_b \\\\\n//    spark.sql(\"CREATE TABLE IF NOT EXISTS cons_aXb AS SELECT * FROM cons_a INTERSECT SELECT * FROM cons_b\")\n//    spark.sql(\"CREATE TABLE IF NOT EXISTS all_branch AS SELECT * FROM branch_a UNION SELECT * FROM branch_b UNION SELECT * FROM branch_c\")\n//    spark.sql(\"CREATE TABLE IF NOT EXISTS cons_abc AS SELECT * FROM cons_a UNION SELECT * FROM cons_b UNION SELECT * FROM cons_c\")\n//\n//    for (x <- 1 to 9) {\n//      spark.sql(s\"CREATE TABLE IF NOT EXISTS b${x}bevs AS SELECT bev FROM all_branch WHERE branch = 'Branch$x'\")\n//    }\n//    for (x <- 1 to 9) {\n//      spark.sql(s\"CREATE TABLE IF NOT EXISTS bevTot$x AS SELECT $x AS branch, COUNT(bev) AS bevTot FROM \" +\n//        s\"b${x}bevs\")\n//    }\n//    for (x <- 1 to 9) {\n//      val c = spark.sql(s\"SELECT $x AS branch, SUM(count) AS cons FROM \" +\n//      s\"b${x}bevs INNER JOIN cons_abc AS c ON c.bev = b${x}bevs.bev\").collect()\n//      val pw = new PrintWriter(new File(s\"input/dumb$x.txt\" ))\n//      pw.write((c(0)(0)).toString + ',' + c(0)(1) + '\\n')\n//      pw.close()\n//    }\n    // for (x <- 1 to 9) {\n    //   spark.sql(s\"CREATE TABLE IF NOT EXISTS consTot$x (branch INT, consTot INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\")\n    //   spark.sql(s\"LOAD DATA LOCAL INPATH 'input/dumb$x.txt' OVERWRITE INTO TABLE consTot$x\")\n    // }\n//    var s = \"CREATE TABLE IF NOT EXISTS bevTotAll AS SELECT * FROM bevTot1 \"\n//    for (x <- 2 to 9) {\n//      s = s + s\"UNION SELECT * FROM bevTot$x \"\n//    }\n//    spark.sql(s)\n\n    // var s2 = \"CREATE TABLE IF NOT EXISTS constotall AS SELECT * FROM constot1 \"\n    // for (x <- 2 to 9) {\n    //   s2 = s2 + s\"UNION SELECT * FROM constot$x \"\n    // }\n    // spark.sql(s2)\n\n    //    bevs common between BranchA and ConscountA \\\\\n//      spark.sql(\"SELECT branch_a.branch, cons_a.bev, cons_a.count FROM branch_a \" +\n//        \"INNER JOIN cons_a ON cons_a.bev = branch_a.bev ORDER BY branch_a.branch, cons_a.bev, cons_a.count\").show()\n\n\n\n    //---------------------------------------------------------------------------------------------------------------\n    //PATRICK'S JUNK:\n    /*\n   spark.sql(\n     \"CREATE TABLE IF NOT EXISTS test (year STRING, total STRING)\" +\n       \"ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\"\n   )\n   spark.sql(\n     \"LOAD DATA LOCAL INPATH 'input/FileName.txt' OVERWRITE INTO TABLE test\"\n   )\n   spark.sql(\"select * from test\").show\n    */\n    /*\n    //2016\n    spark.sql(\"DROP TABLE IF EXISTS crash2016\")\n    spark.sql(\"CREATE TABLE IF NOT EXISTS crash2016(crashType int, age15_19 int, age15_20 int, age16_19 int, age16_20 int, \\n\" +\n      \"age16_24 int, age21_24 int, age60plus int, motorcyle int, pedestrian int, pedalcyclist int, pedalFatal int, pedestrianFatal int, \\n\" +\n      \"relationToRoad int, ruralUrban int, fatals int, schoolBus int, stateNum int, state String, stateCase int, year int) \\n\" +\n      \"ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\")\n    spark.sql(\"LOAD DATA LOCAL INPATH 'input/main/2016vp.csv' OVERWRITE INTO TABLE crash2016\")\n    //val df16 = spark.sql(\"SELECT * FROM crash2017\")\n    val df16 = spark.sql(\"SELECT sum(fatals) as fatalities, year from crash2016 group by year\")\n    //df16.show()\n\n    //2017\n    spark.sql(\"DROP TABLE IF EXISTS crash2017\")\n    spark.sql(\"CREATE TABLE IF NOT EXISTS crash2017(crashType int, age15_19 int, age15_20 int, age16_19 int, age16_20 int, \\n\" +\n      \"age16_24 int, age21_24 int, age60plus int, motorcyle int, pedestrian int, pedalcyclist int, pedalFatal int, pedestrianFatal int, \\n\" +\n      \"relationToRoad int, ruralUrban int, fatals int, schoolBus int, stateNum int, state String, stateCase int, year int) \\n\" +\n      \"ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\")\n    spark.sql(\"LOAD DATA LOCAL INPATH 'input/main/2017vp.csv' OVERWRITE INTO TABLE crash2017\")\n    //val df17 = spark.sql(\"SELECT * FROM crash2017\")\n    val df17 = spark.sql(\"SELECT sum(fatals) as fatalities, year from crash2017 group by year\")\n    //df17.show()\n\n    //2018\n    spark.sql(\"DROP TABLE IF EXISTS crash2018\")\n    spark.sql(\"CREATE TABLE IF NOT EXISTS crash2018(crashType int, age15_19 int, age15_20 int, age16_19 int, age16_20 int, \\n\" +\n      \"age16_24 int, age21_24 int, age60plus int, motorcyle int, pedestrian int, pedalcyclist int, pedalFatal int, pedestrianFatal int, \\n\" +\n      \"relationToRoad int, ruralUrban int, fatals int, schoolBus int, stateNum int, state String, stateCase int, year int) \\n\" +\n      \"ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\")\n    spark.sql(\"LOAD DATA LOCAL INPATH 'input/main/2018vp.csv' OVERWRITE INTO TABLE crash2018\")\n    //val df18 = spark.sql(\"SELECT * FROM crash2018\")\n    val df18 = spark.sql(\"SELECT sum(fatals) as fatalities, year from crash2018 group by year\")\n    //df18.show()\n\n    //2019\n    //val df = spark.read.option(\"header\", true).csv(\"input/2019v2.csv\")\n    spark.sql(\"DROP TABLE IF EXISTS crash2019\")\n    spark.sql(\"CREATE TABLE IF NOT EXISTS crash2019(crashType int, age15_19 int, age15_20 int, age16_19 int, age16_20 int, \\n\" +\n      \"age16_24 int, age21_24 int, age60plus int, motorcyle int, pedestrian int, pedalcyclist int, pedalFatal int, pedestrianFatal int, \\n\" +\n      \"relationToRoad int, ruralUrban int, fatals int, schoolBus int, stateNum int, state String, stateCase int, year int) \\n\" +\n      \"ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\")\n    spark.sql(\"LOAD DATA LOCAL INPATH 'input/main/2019vp.csv' OVERWRITE INTO TABLE crash2019\")\n    //val df = spark.sql(\"SELECT * FROM crash2019\")\n    //val df = spark.sql(\"SELECT sum(fatals), state, year from crash2019 group by year, state\")\n    val df19 = spark.sql(\"SELECT sum(fatals) as fatalities, year from crash2019 group by year\")\n    //df19.show()\n    //df.select(\"state\").distinct.show(57)\n    //df.select(\"fatals\", \"state\", \"year\").show()\n    //df.show()\n    */\n\n    //ALL\n    //ALL YEARS 2016-2019\n    //import spark.implicits._\n    //val rdd = spark.sparkContext.textFile(\"input/main/2016v2.csv,input/main/2017v2.csv,input/main/2018v2.csv,input/main/2019v2.csv\")\n    //val rdd = spark.sparkContext.parallelize(data)\n    //val df = spark.read.option(\"header\", true).csv(\"input/main/*\")\n    //val df = rdd.toDF()\n    //df.show()\n    //df.where(\"STATE = 'Alabama'\").show()\n\n    //spark.sql(\"DROP TABLE IF EXISTS crashData\")\n    //spark.sql(\"CREATE TABLE IF NOT EXISTS crashData(crashType int, age15_19 int, age15_20 int, age16_19 int, age16_20 int, \\n\" +\n    //  \"age16_24 int, age21_24 int, age60plus int, motorcyle int, pedestrian int, pedalcyclist int, pedalFatal int, pedestrianFatal int, \\n\" +\n    //  \"relationToRoad int, ruralUrban int, fatals int, schoolBus int, stateNum int, state String, stateCase int, year int) \\n\" +\n    //  \"ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\")\n    //spark.sql(\"LOAD DATA LOCAL INPATH 'input/main/*' OVERWRITE INTO TABLE crashData\")\n    //val dfAll = spark.sql(\"SELECT * FROM crashData\")\n    //dfAll.show()\n\n    //CREATE TABLE OF ALL DATA\n    //spark.sql(\"DROP TABLE IF EXISTS crashData\")\n    //spark.sql(\"CREATE TABLE IF NOT EXISTS crashData(crashType int, age15_19 int, age15_20 int, age16_19 int, age16_20 int, \\n\" +\n    //  \"age16_24 int, age21_24 int, age60plus int, motorcyle int, pedestrian int, pedalcyclist int, pedalFatal int, pedestrianFatal int, \\n\" +\n    //  \"relationToRoad int, ruralUrban int, fatals int, schoolBus int, stateNum int, state String, stateCase int, year int) \\n\" +\n    //  \"ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\")\n    //spark.sql(\"LOAD DATA LOCAL INPATH 'input/main/*' OVERWRITE INTO TABLE crashData\")\n    //val dfAll = spark.sql(\"SELECT * FROM crashData\")\n    //dfAll.show()\n\n\n    //NEW WAY\n    //spark.sql(\"drop table if exists vehicleStats\")\n    //val df = spark.read.option(\"header\", true).csv(\"input/vehicleStats/*\")\n    //df.show(50)\n    //OLD WAY\n    //spark.sql(\"drop table if exists vehicleStats\")\n    //spark.sql(\"create table if not exists vehicleStats(vehicleType varchar(30), total int, percent double, \\n\" +\n    //\"year int) row format delimited fields terminated by ','\")\n    //spark.sql(\"load data local inpath 'input/vehicleStats/*' overwrite into table vehicleStats\")\n\n    //Read CSV file into a DF\n    //val vDF = spark.read.option(\"header\", true).csv(\"input/vehicleStats/*\")\n    //vDF.write.parquet(\"spark-warehouse/vehiclestats/vehicleStats.parquet\")\n    //vDF.createOrReplaceTempView(\"vehicleStats\")\n    //val dfVehicle = spark.sql(\"select * from vehicleStats\")\n    //dfVehicle.show()\n\n    //vDF.write.parquet(\"vehicleStats.parquet\")\n    //vDF.createOrReplaceTempView(\"vehicleStats\")\n    //val dfVehicle = spark.sql(\"select * from vehicleStats order by vehicleType, year\")\n    //dfVehicle.show(30)\n\n    //import org.apache.spark.storage.StorageLevel\n    //val rdd2 = rdd.persist(StorageLevel.MEMORY_ONLY_SER)\n    //or\n    //val df2 = df.persist(StorageLevel.MEMORY_ONLY_SER)\n\n    //spark.sql(\"drop table if exists crash2016\")\n    //spark.sql(\"drop table if exists crash2017\")\n    //spark.sql(\"drop table if exists crash2018\")\n    //spark.sql(\"drop table if exists crash2019\")\n    //spark.sql(\"drop table if exists test\")\n\n    //END OF PATRICK'S JUNK.\n  }\n\n  def end(): Unit = {\n    println(\"Thank you for your time, hope you enjoyed those queries!\")\n  }\n\n  def chooseN(n: Byte): Byte = {\n    // val temp = readLine()\n    var input: Char = readLine().charAt(0)\n    var inByte: Byte = 0\n    var goodIn: Boolean = false\n\n    n match {\n      case 1 =>\n        println(\n          \"Sorry, but you have to choose '1'... Huh... almost feels like you have no choice at all... OK you can go now. \"\n        );\n        goodIn = true;\n        inByte = 1.toByte\n      case 2 =>\n        while (!goodIn) {\n          input match {\n            case '1' => goodIn = true; inByte = 1.toByte\n            case '2' => goodIn = true; inByte = 2.toByte\n            case _ =>\n              println(\"Sorry, but you have to choose '1', or '2': \");\n              input = readChar()\n          }\n        }\n      case 3 =>\n        while (!goodIn) {\n          input match {\n            case '1' => goodIn = true; inByte = 1.toByte\n            case '2' => goodIn = true; inByte = 2.toByte\n            case '3' => goodIn = true; inByte = 3.toByte\n            case _ =>\n              println(\"Sorry, but you have to choose '1', '2', or '3': \");\n              input = readChar()\n          }\n        }\n      case 4 =>\n        while (!goodIn) {\n          input match {\n            case '1' => goodIn = true; inByte = 1.toByte\n            case '2' => goodIn = true; inByte = 2.toByte\n            case '3' => goodIn = true; inByte = 3.toByte\n            case '4' => goodIn = true; inByte = 4.toByte\n            case _ =>\n              println(\"Sorry, but you have to choose '1', '2', '3', or '4': \");\n              input = readChar()\n          }\n        }\n      case 5 =>\n        while (!goodIn) {\n          input match {\n            case '1' => goodIn = true; inByte = 1.toByte\n            case '2' => goodIn = true; inByte = 2.toByte\n            case '3' => goodIn = true; inByte = 3.toByte\n            case '4' => goodIn = true; inByte = 4.toByte\n            case '5' => goodIn = true; inByte = 5.toByte\n            case _ =>\n              println(\n                \"Sorry, but you have to choose '1', '2', '3', '4', or '5': \"\n              ); input = readChar()\n          }\n        }\n      case 6 =>\n        while (!goodIn) {\n          input match {\n            case '1' => goodIn = true; inByte = 1.toByte\n            case '2' => goodIn = true; inByte = 2.toByte\n            case '3' => goodIn = true; inByte = 3.toByte\n            case '4' => goodIn = true; inByte = 4.toByte\n            case '5' => goodIn = true; inByte = 5.toByte\n            case '6' => goodIn = true; inByte = 6.toByte\n            case _ =>\n              println(\n                \"Sorry, but you have to choose '1', '2', '3', '4', '5', or '6': \"\n              ); input = readChar()\n          }\n        }\n      case 7 =>\n        while (!goodIn) {\n          input match {\n            case '1' => goodIn = true; inByte = 1.toByte\n            case '2' => goodIn = true; inByte = 2.toByte\n            case '3' => goodIn = true; inByte = 3.toByte\n            case '4' => goodIn = true; inByte = 4.toByte\n            case '5' => goodIn = true; inByte = 5.toByte\n            case '6' => goodIn = true; inByte = 6.toByte\n            case '7' => goodIn = true; inByte = 7.toByte\n            case _ =>\n              println(\n                \"Sorry, but you have to choose '1', '2', '3', '4', '5', '6', or '7': \"\n              ); input = readChar()\n          }\n        }\n    }\n    inByte\n  }\n\n  def menuLev2(options: List[String]): Unit = {\n    val menu2 = new MyMenu(options)\n    var continue = true\n    while(continue) {\n      menu2.printMenu()\n      //put input from user into a var called in\n      val in = chooseN(options.length.toByte)\n      //select the user's option\n      val option = menu2.selectOption(in)\n      option match{\n        case \"A\" => println(\"Jessica's topic.\")\n\n        case \"B\" => //GRAPH TRENDS OF FATALITIES IN ENTIRE U.S. FOR 4 YEARS:\n          //CREATE TABLE OF ALL DATA\n          //val peopleDF = spark.read.option(\"input/vehicleStats/*\")\n          val aDF = spark.read.option(\"header\", true).csv(\"input/main/*\").toDF()\n          // DataFrames can be saved as Parquet files, maintaining the schema information\n          aDF.write.mode(\"overwrite\").parquet(\"spark-warehouse/usCrashes.parquet\")\n          // Read in the parquet file created above\n          // Parquet files are self-describing so the schema is preserved\n          // The result of loading a Parquet file is also a DataFrame\n          val parquetFileDF = spark.read.parquet(\"spark-warehouse/usCrashes.parquet\")\n          // Parquet files can also be used to create a temporary view and then used in SQL statements\n          parquetFileDF.createOrReplaceTempView(\"crashData\")\n          val dfAll = spark.sql(\"select * from crashData\")\n          //dfAll.show()\n          //Optimization\n          dfAll.persist(StorageLevel.MEMORY_ONLY_SER)\n\n          //Graph the trend of fatalities in the entire USA\n          println(\"Trend of fatalities in the entire USA from 2016 to 2019:\")\n          val dfAllUS = spark.sql(\"SELECT sum(fatals) as fatalities, year from crashData group by year order by year\")\n          dfAllUS.show()\n          //Optimization\n          dfAllUS.persist(StorageLevel.MEMORY_ONLY_SER)\n          /*\n          df.write\n          .format(\"csv\")\n          .option(\"header\", true)\n          .mode(\"overwrite\")\n          .save(\"hdfs://localhost:9000/user/patrickbrown/future.csv\")\n          */\n\n        case \"C\" => //GRAPH TRENDS OF FATALITIES IN EACH STATE:\n          //CREATE TABLE OF ALL DATA\n          //val peopleDF = spark.read.option(\"input/vehicleStats/*\")\n          val aDF = spark.read.option(\"header\", true).csv(\"input/main/*\").toDF()\n          // DataFrames can be saved as Parquet files, maintaining the schema information\n          aDF.write.mode(\"overwrite\").parquet(\"spark-warehouse/usCrashes.parquet\")\n          // Read in the parquet file created above\n          // Parquet files are self-describing so the schema is preserved\n          // The result of loading a Parquet file is also a DataFrame\n          val parquetFileDF = spark.read.parquet(\"spark-warehouse/usCrashes.parquet\")\n          // Parquet files can also be used to create a temporary view and then used in SQL statements\n          parquetFileDF.createOrReplaceTempView(\"crashData\")\n          val dfAll = spark.sql(\"select * from crashData\")\n          //dfAll.show()\n          //Optimization\n          dfAll.persist(StorageLevel.MEMORY_ONLY_SER)\n\n          //Graph the trend of fatalities in individual states\n          println(\"Trend of fatalities in individual states from 2016 to 2019:\")\n          val dfState = spark.sql(\"SELECT sum(fatals) as fatalities, year, state from crashData group by state, year \\n\" +\n            \"order by state, year\")\n          dfState.show()\n          //Optimization\n          dfState.persist(StorageLevel.MEMORY_ONLY_SER)\n          /*\n          df.write\n          .format(\"csv\")\n          .option(\"header\", true)\n          .mode(\"overwrite\")\n          .save(\"hdfs://localhost:9000/user/patrickbrown/future.csv\")\n          */\n\n        case \"D\" => //WHICH STATES ARE THE SAFEST?\n          //CREATE TABLE OF ALL DATA\n          //val peopleDF = spark.read.option(\"input/vehicleStats/*\")\n          val aDF = spark.read.option(\"header\", true).csv(\"input/main/*\").toDF()\n          // DataFrames can be saved as Parquet files, maintaining the schema information\n          aDF.write.mode(\"overwrite\").parquet(\"spark-warehouse/usCrashes.parquet\")\n          // Read in the parquet file created above\n          // Parquet files are self-describing so the schema is preserved\n          // The result of loading a Parquet file is also a DataFrame\n          val parquetFileDF = spark.read.parquet(\"spark-warehouse/usCrashes.parquet\")\n          // Parquet files can also be used to create a temporary view and then used in SQL statements\n          parquetFileDF.createOrReplaceTempView(\"crashData\")\n          val dfAll = spark.sql(\"select * from crashData\")\n          //Optimization\n          dfAll.persist(StorageLevel.MEMORY_ONLY_SER)\n\n          //Trend by year for all states\n          //States with highest crashes every year\n          println(\"States with highest crash fatality numbers every year: \")\n          val dfState2016 = spark.sql(\"SELECT sum(fatals) as fatalities, year, state from crashData where year = 2016 \\n\" +\n            \"group by state, year order by fatalities DESC LIMIT 8\")\n          val dfState2017 = spark.sql(\"SELECT sum(fatals) as fatalities, year, state from crashData where year = 2017\\n\" +\n            \"group by state, year order by fatalities DESC LIMIT 8\")\n          val dfState2018 = spark.sql(\"SELECT sum(fatals) as fatalities, year, state from crashData where year = 2018 \\n\" +\n            \"group by state, year order by fatalities DESC LIMIT 8\")\n          val dfState2019 = spark.sql(\"SELECT sum(fatals) as fatalities, year, state from crashData where year = 2019 \\n\" +\n            \"group by state, year order by fatalities DESC LIMIT 8\")\n          dfState2016.show()\n          dfState2017.show()\n          dfState2018.show()\n          dfState2019.show()\n          //Optimization\n          dfState2016.persist(StorageLevel.MEMORY_ONLY_SER)\n          dfState2016.persist(StorageLevel.MEMORY_ONLY_SER)\n          dfState2016.persist(StorageLevel.MEMORY_ONLY_SER)\n          dfState2016.persist(StorageLevel.MEMORY_ONLY_SER)\n          //States with lowest crashes every year\n          println(\"States with lowest crash fatality numbers every year: \")\n          val state2016down = spark.sql(\"SELECT sum(fatals) as fatalities, year, state from crashData where year = 2016 \\n\" +\n            \"group by state, year order by fatalities LIMIT 8\")\n          val state2017down = spark.sql(\"SELECT sum(fatals) as fatalities, year, state from crashData where year = 2017\\n\" +\n            \"group by state, year order by fatalities LIMIT 8\")\n          val state2018down = spark.sql(\"SELECT sum(fatals) as fatalities, year, state from crashData where year = 2018 \\n\" +\n            \"group by state, year order by fatalities LIMIT 8\")\n          val state2019down = spark.sql(\"SELECT sum(fatals) as fatalities, year, state from crashData where year = 2019 \\n\" +\n            \"group by state, year order by fatalities LIMIT 8\")\n          state2016down.show()\n          state2017down.show()\n          state2018down.show()\n          state2019down.show()\n          //Optimization\n          dfState2016.persist(StorageLevel.MEMORY_ONLY_SER)\n          dfState2016.persist(StorageLevel.MEMORY_ONLY_SER)\n          dfState2016.persist(StorageLevel.MEMORY_ONLY_SER)\n          dfState2016.persist(StorageLevel.MEMORY_ONLY_SER)\n\n          //See what type of vehicle led to the most crashes.\n          println(\"Here are the stats for different vehicles: \")\n          //val peopleDF = spark.read.option(\"input/vehicleStats/*\")\n          val vDF = spark.read.option(\"header\", true).csv(\"input/vehicleStats/*\").toDF()\n          // DataFrames can be saved as Parquet files, maintaining the schema information\n          vDF.write.mode(\"overwrite\").parquet(\"spark-warehouse/vehicle.parquet\")\n          // Read in the parquet file created above\n          // Parquet files are self-describing so the schema is preserved\n          // The result of loading a Parquet file is also a DataFrame\n          val parquetDF = spark.read.parquet(\"spark-warehouse/vehicle.parquet\")\n          // Parquet files can also be used to create a temporary view and then used in SQL statements\n          parquetDF.createOrReplaceTempView(\"vehicleParquetFile\")\n          val x = spark.sql(\"select * from vehicleParquetFile order by Year, VehicleType\")\n          x.show(28)\n          //Optimization\n          x.persist(StorageLevel.MEMORY_ONLY_SER)\n          /*\n          df.write\n          .format(\"csv\")\n          .option(\"header\", true)\n          .mode(\"overwrite\")\n          .save(\"hdfs://localhost:9000/user/patrickbrown/future.csv\")\n          */\n\n        case \"E\" => println(\"Justis' topic.\")\n        case \"F\" => println(\"Justis' topic.\")\n        case \"G\" => println(\"Jonathan's topic.\")\n        case \"H\" => println(\"Jonathan's topic.\")\n        case b => continue = false\n      }\n    }\n  }\n}\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/main/scala/Utilities.scala b/src/main/scala/Utilities.scala
--- a/src/main/scala/Utilities.scala	(revision 02f27efa4fc8fea058df1188721511ebf35cb2fe)
+++ b/src/main/scala/Utilities.scala	(date 1645346459391)
@@ -178,17 +178,6 @@
     //val dfAll = spark.sql("SELECT * FROM crashData")
     //dfAll.show()
 
-    //CREATE TABLE OF ALL DATA
-    //spark.sql("DROP TABLE IF EXISTS crashData")
-    //spark.sql("CREATE TABLE IF NOT EXISTS crashData(crashType int, age15_19 int, age15_20 int, age16_19 int, age16_20 int, \n" +
-    //  "age16_24 int, age21_24 int, age60plus int, motorcyle int, pedestrian int, pedalcyclist int, pedalFatal int, pedestrianFatal int, \n" +
-    //  "relationToRoad int, ruralUrban int, fatals int, schoolBus int, stateNum int, state String, stateCase int, year int) \n" +
-    //  "ROW FORMAT DELIMITED FIELDS TERMINATED BY ','")
-    //spark.sql("LOAD DATA LOCAL INPATH 'input/main/*' OVERWRITE INTO TABLE crashData")
-    //val dfAll = spark.sql("SELECT * FROM crashData")
-    //dfAll.show()
-
-
     //NEW WAY
     //spark.sql("drop table if exists vehicleStats")
     //val df = spark.read.option("header", true).csv("input/vehicleStats/*")
Index: .idea/modules/project.P2-build.iml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<module external.linked.project.id=\"P2-build\" external.linked.project.path=\"$MODULE_DIR$/../../project\" external.root.project.path=\"$MODULE_DIR$/../..\" external.system.id=\"SBT\" sbt.imports=\"SUB:DOLLAR1cf86c658605f07e7b69.`root`, _root_.sbt.Keys._, _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.JUnitXmlReportPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.sbt._, _root_.sbt.nio.Keys._, _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.scala.xml.{TopScope=&amp;amp;amp;gt;SUB:DOLLARscope}\" sbt.resolvers=\"$USER_HOME$/.ivy2/cache|ivy|Local cache, https://repo1.maven.org/maven2/|maven|public\" type=\"SBT_MODULE\" version=\"4\">\n  <component name=\"NewModuleRootManager\">\n    <output url=\"file://$MODULE_DIR$/../../project/target/idea-classes\" />\n    <output-test url=\"file://$MODULE_DIR$/../../project/target/idea-test-classes\" />\n    <exclude-output />\n    <content url=\"file://$MODULE_DIR$/../../project\">\n      <sourceFolder url=\"file://$MODULE_DIR$/../../project\" isTestSource=\"false\" />\n      <excludeFolder url=\"file://$MODULE_DIR$/../../project/project/target\" />\n      <excludeFolder url=\"file://$MODULE_DIR$/../../project/target\" />\n    </content>\n    <orderEntry type=\"inheritedJdk\" />\n    <orderEntry type=\"sourceFolder\" forTests=\"false\" />\n    <orderEntry type=\"module-library\" scope=\"PROVIDED\">\n      <library name=\"sbt: sbt-1.5.8\">\n        <CLASSES>\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/lib/scala-compiler.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/lib/scala-library.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/lib/scala-reflect.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/lib/scala-xml_2.12-1.0.6.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/actions_2.12-1.5.8.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/caffeine-2.8.5.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/checker-qual-3.4.1.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/collections_2.12-1.5.8.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/command_2.12-1.5.8.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/compiler-bridge_2.12-1.5.7.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/compiler-interface-1.5.7.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/completion_2.12-1.5.8.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/config-1.3.3.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/core-macros_2.12-1.5.8.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/disruptor-3.4.2.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/error_prone_annotations-2.4.0.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/file-tree-views-2.1.6.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/gigahorse-core_2.12-0.5.0.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/gigahorse-okhttp_2.12-0.5.0.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/io_2.12-1.5.1.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/ipcsocket-1.3.1.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/ivy-2.3.0-sbt-fbc4f586aeeb1591710b14eb4f41b94880dcd745.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/jansi-2.1.0.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/jline-2.14.7-sbt-a1b0ffbb8f64bb820f4f84a0c07a0c0964507493.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/jline-builtins-3.19.0.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/jline-reader-3.19.0.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/jline-style-3.19.0.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/jline-terminal-3.19.0.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/jline-terminal-jansi-3.19.0.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/jline-terminal-jna-3.19.0.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/jna-5.5.0.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/jna-platform-5.5.0.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/jsch-0.1.54.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/launcher-interface-1.3.3.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/librarymanagement-core_2.12-1.5.3.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/librarymanagement-ivy_2.12-1.5.3.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/lm-coursier-shaded_2.12-2.0.8.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/log4j-api-2.17.0.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/log4j-core-2.17.0.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/log4j-slf4j-impl-2.17.0.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/logic_2.12-1.5.8.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/main-settings_2.12-1.5.8.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/main_2.12-1.5.8.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/okhttp-3.14.2.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/okhttp-urlconnection-3.7.0.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/okio-1.17.2.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/protocol_2.12-1.5.8.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/reactive-streams-1.0.2.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/run_2.12-1.5.8.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/sbinary_2.12-0.5.1.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/sbt-1.5.8.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/scala-collection-compat_2.12-2.4.2.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/scala-compiler-2.12.14.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/scala-library-2.12.14.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/scala-parser-combinators_2.12-1.1.2.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/scala-reflect-2.12.14.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/scala-xml_2.12-1.3.0.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/scripted-plugin_2.12-1.5.8.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/shaded-jawn-parser_2.12-0.9.1.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/shaded-scalajson_2.12-1.0.0-M4.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/sjson-new-core_2.12-0.9.1.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/sjson-new-murmurhash_2.12-0.9.1.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/sjson-new-scalajson_2.12-0.9.1.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/slf4j-api-1.7.26.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/ssl-config-core_2.12-0.4.0.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/task-system_2.12-1.5.8.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/tasks_2.12-1.5.8.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/template-resolver-0.1.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/test-agent-1.5.8.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/test-interface-1.0.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/testing_2.12-1.5.8.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/util-cache_2.12-1.5.8.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/util-control_2.12-1.5.8.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/util-interface-1.5.8.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/util-logging_2.12-1.5.8.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/util-position_2.12-1.5.8.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/util-relation_2.12-1.5.8.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/util-tracking_2.12-1.5.8.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/zero-allocation-hashing-0.10.1.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/zinc-apiinfo_2.12-1.5.7.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/zinc-classfile_2.12-1.5.7.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/zinc-classpath_2.12-1.5.7.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/zinc-compile-core_2.12-1.5.7.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/zinc-compile_2.12-1.5.7.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/zinc-core_2.12-1.5.7.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/zinc-lm-integration_2.12-1.5.8.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/zinc-persist-core-assembly-1.5.7.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/zinc-persist_2.12-1.5.7.jar!/\" />\n          <root url=\"jar://$USER_HOME$/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.8/zinc_2.12-1.5.7.jar!/\" />\n        </CLASSES>\n        <JAVADOC />\n        <SOURCES>\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/eed3si9n/gigahorse-core_2.12/0.5.0/gigahorse-core_2.12-0.5.0-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/eed3si9n/gigahorse-okhttp_2.12/0.5.0/gigahorse-okhttp_2.12-0.5.0-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/eed3si9n/shaded-jawn-parser_2.12/0.9.1/shaded-jawn-parser_2.12-0.9.1-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/eed3si9n/shaded-scalajson_2.12/1.0.0-M4/shaded-scalajson_2.12-1.0.0-M4-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/eed3si9n/sjson-new-core_2.12/0.9.1/sjson-new-core_2.12-0.9.1-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/eed3si9n/sjson-new-murmurhash_2.12/0.9.1/sjson-new-murmurhash_2.12-0.9.1-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/eed3si9n/sjson-new-scalajson_2.12/0.9.1/sjson-new-scalajson_2.12-0.9.1-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/github/ben-manes/caffeine/caffeine/2.8.5/caffeine-2.8.5-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/google/errorprone/error_prone_annotations/2.4.0/error_prone_annotations-2.4.0-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/jcraft/jsch/0.1.54/jsch-0.1.54-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lmax/disruptor/3.4.2/disruptor-3.4.2-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/squareup/okhttp3/okhttp-urlconnection/3.7.0/okhttp-urlconnection-3.7.0-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/squareup/okhttp3/okhttp/3.14.2/okhttp-3.14.2-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/squareup/okio/okio/1.17.2/okio-1.17.2-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/swoval/file-tree-views/2.1.6/file-tree-views-2.1.6-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/config/1.3.3/config-1.3.3-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/ssl-config-core_2.12/0.4.0/ssl-config-core_2.12-0.4.0-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/get-coursier/lm-coursier-shaded_2.12/2.0.8/lm-coursier-shaded_2.12-2.0.8-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/net/java/dev/jna/jna-platform/5.5.0/jna-platform-5.5.0-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/net/java/dev/jna/jna/5.5.0/jna-5.5.0-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/net/openhft/zero-allocation-hashing/0.10.1/zero-allocation-hashing-0.10.1-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/logging/log4j/log4j-api/2.17.0/log4j-api-2.17.0-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/logging/log4j/log4j-core/2.17.0/log4j-core-2.17.0-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/logging/log4j/log4j-slf4j-impl/2.17.0/log4j-slf4j-impl-2.17.0-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/checkerframework/checker-qual/3.4.1/checker-qual-3.4.1-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/fusesource/jansi/jansi/2.1.0/jansi-2.1.0-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/jline/jline-builtins/3.19.0/jline-builtins-3.19.0-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/jline/jline-reader/3.19.0/jline-reader-3.19.0-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/jline/jline-style/3.19.0/jline-style-3.19.0-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/jline/jline-terminal-jansi/3.19.0/jline-terminal-jansi-3.19.0-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/jline/jline-terminal-jna/3.19.0/jline-terminal-jna-3.19.0-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/jline/jline-terminal/3.19.0/jline-terminal-3.19.0-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.2/reactive-streams-1.0.2-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-collection-compat_2.12/2.4.2/scala-collection-compat_2.12-2.4.2-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.12/1.1.2/scala-parser-combinators_2.12-1.1.2-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.3.0/scala-xml_2.12-1.3.0-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.12.14/scala-compiler-2.12.14-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.14/scala-library-2.12.14-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.14/scala-reflect-2.12.14-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/actions_2.12/1.5.8/actions_2.12-1.5.8-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/collections_2.12/1.5.8/collections_2.12-1.5.8-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/command_2.12/1.5.8/command_2.12-1.5.8-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/compiler-bridge_2.12/1.5.7/compiler-bridge_2.12-1.5.7-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/compiler-interface/1.5.7/compiler-interface-1.5.7-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/completion_2.12/1.5.8/completion_2.12-1.5.8-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/core-macros_2.12/1.5.8/core-macros_2.12-1.5.8-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/io_2.12/1.5.1/io_2.12-1.5.1-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/ipcsocket/ipcsocket/1.3.1/ipcsocket-1.3.1-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/ivy/ivy/2.3.0-sbt-fbc4f586aeeb1591710b14eb4f41b94880dcd745/ivy-2.3.0-sbt-fbc4f586aeeb1591710b14eb4f41b94880dcd745-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/jline/jline/2.14.7-sbt-a1b0ffbb8f64bb820f4f84a0c07a0c0964507493/jline-2.14.7-sbt-a1b0ffbb8f64bb820f4f84a0c07a0c0964507493-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/launcher-interface/1.3.3/launcher-interface-1.3.3-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/librarymanagement-core_2.12/1.5.3/librarymanagement-core_2.12-1.5.3-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/librarymanagement-ivy_2.12/1.5.3/librarymanagement-ivy_2.12-1.5.3-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/logic_2.12/1.5.8/logic_2.12-1.5.8-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/main-settings_2.12/1.5.8/main-settings_2.12-1.5.8-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/main_2.12/1.5.8/main_2.12-1.5.8-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/protocol_2.12/1.5.8/protocol_2.12-1.5.8-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/run_2.12/1.5.8/run_2.12-1.5.8-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/sbinary_2.12/0.5.1/sbinary_2.12-0.5.1-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/sbt/1.5.8/sbt-1.5.8-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/scripted-plugin_2.12/1.5.8/scripted-plugin_2.12-1.5.8-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/task-system_2.12/1.5.8/task-system_2.12-1.5.8-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/tasks_2.12/1.5.8/tasks_2.12-1.5.8-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/template-resolver/0.1/template-resolver-0.1-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/test-agent/1.5.8/test-agent-1.5.8-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/test-interface/1.0/test-interface-1.0-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/testing_2.12/1.5.8/testing_2.12-1.5.8-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/util-cache_2.12/1.5.8/util-cache_2.12-1.5.8-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/util-control_2.12/1.5.8/util-control_2.12-1.5.8-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/util-interface/1.5.8/util-interface-1.5.8-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/util-logging_2.12/1.5.8/util-logging_2.12-1.5.8-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/util-position_2.12/1.5.8/util-position_2.12-1.5.8-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/util-relation_2.12/1.5.8/util-relation_2.12-1.5.8-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/util-tracking_2.12/1.5.8/util-tracking_2.12-1.5.8-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/zinc-apiinfo_2.12/1.5.7/zinc-apiinfo_2.12-1.5.7-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/zinc-classfile_2.12/1.5.7/zinc-classfile_2.12-1.5.7-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/zinc-classpath_2.12/1.5.7/zinc-classpath_2.12-1.5.7-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/zinc-compile-core_2.12/1.5.7/zinc-compile-core_2.12-1.5.7-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/zinc-compile_2.12/1.5.7/zinc-compile_2.12-1.5.7-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/zinc-core_2.12/1.5.7/zinc-core_2.12-1.5.7-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/zinc-lm-integration_2.12/1.5.8/zinc-lm-integration_2.12-1.5.8-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/zinc-persist-core-assembly/1.5.7/zinc-persist-core-assembly-1.5.7-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/zinc-persist_2.12/1.5.7/zinc-persist_2.12-1.5.7-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/zinc_2.12/1.5.7/zinc_2.12-1.5.7-sources.jar!/\" />\n          <root url=\"jar://$USER_HOME$/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.26/slf4j-api-1.7.26-sources.jar!/\" />\n        </SOURCES>\n      </library>\n    </orderEntry>\n  </component>\n</module>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/modules/project.P2-build.iml b/.idea/modules/project.P2-build.iml
--- a/.idea/modules/project.P2-build.iml	(revision 02f27efa4fc8fea058df1188721511ebf35cb2fe)
+++ b/.idea/modules/project.P2-build.iml	(date 1645346146881)
@@ -1,5 +1,5 @@
 <?xml version="1.0" encoding="UTF-8"?>
-<module external.linked.project.id="P2-build" external.linked.project.path="$MODULE_DIR$/../../project" external.root.project.path="$MODULE_DIR$/../.." external.system.id="SBT" sbt.imports="SUB:DOLLAR1cf86c658605f07e7b69.`root`, _root_.sbt.Keys._, _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.JUnitXmlReportPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.sbt._, _root_.sbt.nio.Keys._, _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.scala.xml.{TopScope=&amp;amp;amp;gt;SUB:DOLLARscope}" sbt.resolvers="$USER_HOME$/.ivy2/cache|ivy|Local cache, https://repo1.maven.org/maven2/|maven|public" type="SBT_MODULE" version="4">
+<module external.linked.project.id="P2-build" external.linked.project.path="$MODULE_DIR$/../../project" external.root.project.path="$MODULE_DIR$/../.." external.system.id="SBT" sbt.imports="SUB:DOLLAR1cf86c658605f07e7b69.`root`, _root_.sbt.Keys._, _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.JUnitXmlReportPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.sbt._, _root_.sbt.nio.Keys._, _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.scala.xml.{TopScope=&amp;amp;amp;amp;gt;SUB:DOLLARscope}" sbt.resolvers="$USER_HOME$/.ivy2/cache|ivy|Local cache, https://repo1.maven.org/maven2/|maven|public" type="SBT_MODULE" version="4">
   <component name="NewModuleRootManager">
     <output url="file://$MODULE_DIR$/../../project/target/idea-classes" />
     <output-test url="file://$MODULE_DIR$/../../project/target/idea-test-classes" />
@@ -195,4 +195,8 @@
       </library>
     </orderEntry>
   </component>
+  <component name="SbtModule">
+    <option name="buildForURI" value="file:$MODULE_DIR$/../../" />
+    <option name="imports" value="SUB:DOLLAR1cf86c658605f07e7b69.`root`, _root_.sbt.Keys._, _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.JUnitXmlReportPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.sbt._, _root_.sbt.nio.Keys._, _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.scala.xml.{TopScope=&gt;SUB:DOLLARscope}" />
+  </component>
 </module>
\ No newline at end of file
Index: .idea/workspace.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project version=\"4\">\n  <component name=\"AutoImportSettings\">\n    <option name=\"autoReloadType\" value=\"SELECTIVE\" />\n  </component>\n  <component name=\"ChangeListManager\">\n    <list default=\"true\" id=\"241ceada-61af-4624-bfc1-43194785b652\" name=\"Changes\" comment=\"Committing old Q3\">\n      <change beforePath=\"$PROJECT_DIR$/.idea/modules/project.P2-build.iml\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/.idea/modules/project.P2-build.iml\" afterDir=\"false\" />\n    </list>\n    <option name=\"SHOW_DIALOG\" value=\"false\" />\n    <option name=\"HIGHLIGHT_CONFLICTS\" value=\"true\" />\n    <option name=\"HIGHLIGHT_NON_ACTIVE_CHANGELIST\" value=\"false\" />\n    <option name=\"LAST_RESOLUTION\" value=\"IGNORE\" />\n  </component>\n  <component name=\"CodeStyleSettingsInfer\">\n    <option name=\"done\" value=\"true\" />\n  </component>\n  <component name=\"ExternalProjectsData\">\n    <projectState path=\"$PROJECT_DIR$\">\n      <ProjectState />\n    </projectState>\n  </component>\n  <component name=\"ExternalProjectsManager\">\n    <system id=\"SBT\">\n      <state>\n        <projects_view>\n          <tree_state>\n            <expand>\n              <path>\n                <item name=\"\" type=\"6a2764b6:ExternalProjectsStructure$RootNode\" />\n                <item name=\"P2\" type=\"f1a62948:ProjectNode\" />\n              </path>\n            </expand>\n            <select />\n          </tree_state>\n        </projects_view>\n      </state>\n    </system>\n  </component>\n  <component name=\"Git.Settings\">\n    <option name=\"RECENT_BRANCH_BY_REPOSITORY\">\n      <map>\n        <entry key=\"$PROJECT_DIR$\" value=\"dev\" />\n      </map>\n    </option>\n    <option name=\"RECENT_GIT_ROOT_PATH\" value=\"$PROJECT_DIR$\" />\n    <option name=\"RESET_MODE\" value=\"HARD\" />\n  </component>\n  <component name=\"MarkdownSettingsMigration\">\n    <option name=\"stateVersion\" value=\"1\" />\n  </component>\n  <component name=\"ProjectCodeStyleSettingsMigration\">\n    <option name=\"version\" value=\"2\" />\n  </component>\n  <component name=\"ProjectId\" id=\"259uHj71rZxPf595w8sEy0DqVtu\" />\n  <component name=\"ProjectViewState\">\n    <option name=\"hideEmptyMiddlePackages\" value=\"true\" />\n    <option name=\"showLibraryContents\" value=\"true\" />\n  </component>\n  <component name=\"PropertiesComponent\">\n    <property name=\"RunOnceActivity.OpenProjectViewOnStart\" value=\"true\" />\n    <property name=\"RunOnceActivity.ShowReadmeOnStart\" value=\"true\" />\n    <property name=\"last_opened_file_path\" value=\"$PROJECT_DIR$/input/main\" />\n    <property name=\"project.structure.last.edited\" value=\"Project\" />\n    <property name=\"project.structure.proportion\" value=\"0.17\" />\n    <property name=\"project.structure.side.proportion\" value=\"0.2\" />\n    <property name=\"settings.editor.selected.configurable\" value=\"preferences.lookFeel\" />\n  </component>\n  <component name=\"RecentsManager\">\n    <key name=\"CopyFile.RECENT_KEYS\">\n      <recent name=\"$PROJECT_DIR$/input/main\" />\n    </key>\n    <key name=\"MoveFile.RECENT_KEYS\">\n      <recent name=\"$PROJECT_DIR$/input/main\" />\n    </key>\n  </component>\n  <component name=\"RunManager\">\n    <configuration name=\"P2\" type=\"Application\" factoryName=\"Application\" temporary=\"true\">\n      <option name=\"MAIN_CLASS_NAME\" value=\"P2\" />\n      <module name=\"P2\" />\n      <method v=\"2\">\n        <option name=\"Make\" enabled=\"true\" />\n      </method>\n    </configuration>\n    <recent_temporary>\n      <list>\n        <item itemvalue=\"Application.P2\" />\n        <item itemvalue=\"Application.P2\" />\n        <item itemvalue=\"Application.P2\" />\n        <item itemvalue=\"Application.P2\" />\n        <item itemvalue=\"Application.P2\" />\n      </list>\n    </recent_temporary>\n  </component>\n  <component name=\"SpellCheckerSettings\" RuntimeDictionaries=\"0\" Folders=\"0\" CustomDictionaries=\"0\" DefaultDictionary=\"application-level\" UseSingleDictionary=\"true\" transferred=\"true\" />\n  <component name=\"TaskManager\">\n    <task active=\"true\" id=\"Default\" summary=\"Default task\">\n      <changelist id=\"241ceada-61af-4624-bfc1-43194785b652\" name=\"Changes\" comment=\"\" />\n      <created>1644951543265</created>\n      <option name=\"number\" value=\"Default\" />\n      <option name=\"presentableId\" value=\"Default\" />\n      <updated>1644951543265</updated>\n    </task>\n    <task id=\"LOCAL-00001\" summary=\"Inserted the car crash data from the csv files into dataframes and created tables for the fatality trends of the entire US and the individual states.\">\n      <created>1645067240600</created>\n      <option name=\"number\" value=\"00001\" />\n      <option name=\"presentableId\" value=\"LOCAL-00001\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1645067240601</updated>\n    </task>\n    <task id=\"LOCAL-00002\" summary=\"Moved my queries to the UI (queries B, C, and D).\">\n      <created>1645126249730</created>\n      <option name=\"number\" value=\"00002\" />\n      <option name=\"presentableId\" value=\"LOCAL-00002\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1645126249730</updated>\n    </task>\n    <task id=\"LOCAL-00003\" summary=\"All dataframes with data from CSV files have now been converted to parquet files for me to query on. I have also persisted all dataframes to serialized storage levels for optimization.\">\n      <created>1645142513887</created>\n      <option name=\"number\" value=\"00003\" />\n      <option name=\"presentableId\" value=\"LOCAL-00003\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1645142513887</updated>\n    </task>\n    <task id=\"LOCAL-00004\" summary=\"Fixed my code in the junk function.\">\n      <created>1645160383339</created>\n      <option name=\"number\" value=\"00004\" />\n      <option name=\"presentableId\" value=\"LOCAL-00004\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1645160383339</updated>\n    </task>\n    <task id=\"LOCAL-00005\" summary=\"Moved some code to the Utilities object's junk function.\">\n      <created>1645195615862</created>\n      <option name=\"number\" value=\"00005\" />\n      <option name=\"presentableId\" value=\"LOCAL-00005\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1645195615863</updated>\n    </task>\n    <option name=\"localTasksCounter\" value=\"6\" />\n    <servers />\n  </component>\n  <component name=\"Vcs.Log.Tabs.Properties\">\n    <option name=\"TAB_STATES\">\n      <map>\n        <entry key=\"MAIN\">\n          <value>\n            <State />\n          </value>\n        </entry>\n      </map>\n    </option>\n  </component>\n  <component name=\"VcsManagerConfiguration\">\n    <MESSAGE value=\"Inserted the car crash data from the csv files into dataframes and created tables for the fatality trends of the entire US and the individual states.\" />\n    <MESSAGE value=\"Moved my queries to the UI (queries B, C, and D).\" />\n    <MESSAGE value=\"All dataframes with data from CSV files have now been converted to parquet files for me to query on. I have also persisted all dataframes to serialized storage levels for optimization.\" />\n    <MESSAGE value=\"Fixed my code in the junk function.\" />\n    <MESSAGE value=\"Moved some code to the Utilities object's junk function.\" />\n    <option name=\"LAST_COMMIT_MESSAGE\" value=\"Moved some code to the Utilities object's junk function.\" />\n  </component>\n  <component name=\"com.intellij.coverage.CoverageDataManagerImpl\">\n    <SUITE FILE_PATH=\"coverage/P2$P2.ic\" NAME=\"P2 Coverage Results\" MODIFIED=\"1645045264462\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"idea\" COVERAGE_BY_TEST_ENABLED=\"false\" COVERAGE_TRACING_ENABLED=\"false\" />\n  </component>\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/workspace.xml b/.idea/workspace.xml
--- a/.idea/workspace.xml	(revision 02f27efa4fc8fea058df1188721511ebf35cb2fe)
+++ b/.idea/workspace.xml	(date 1645346459593)
@@ -6,6 +6,8 @@
   <component name="ChangeListManager">
     <list default="true" id="241ceada-61af-4624-bfc1-43194785b652" name="Changes" comment="Committing old Q3">
       <change beforePath="$PROJECT_DIR$/.idea/modules/project.P2-build.iml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/modules/project.P2-build.iml" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/.idea/workspace.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/workspace.xml" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/src/main/scala/Utilities.scala" beforeDir="false" afterPath="$PROJECT_DIR$/src/main/scala/Utilities.scala" afterDir="false" />
     </list>
     <option name="SHOW_DIALOG" value="false" />
     <option name="HIGHLIGHT_CONFLICTS" value="true" />
